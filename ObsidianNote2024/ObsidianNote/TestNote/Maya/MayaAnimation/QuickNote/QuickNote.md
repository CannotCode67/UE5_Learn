Exporting From Maya to Unity 

When we export animation to Unity, we first need to make sure in Unity we have the model that is used for creating the animation in Maya. If not, we need to export the model first, that means exporting the geometry and the skeleton, both in their default pose. Then, we export the animation by exporting the skeleton with animation data baked into it. This way, we can save memory. The drawback is in Unity, we would have to assign the model to the animation clip to preview it. If we export the animation by exporting skeleton and the geometry, we can see the animation preview directly in Unity, but we would have each animation clip carrying one geometry data, and that's just wasting a lot of memory. 

If the model is in a form of multiple meshes in Maya, we cannot just export all of them to Unity, because the animation won't work. We have to combine all the meshes with the setting of combine skin weight checked. That's for the mesh. For the rig or skeleton, it seems Unity doesn't care if the skeleton is in one hierarchy. For example, the knee joint is not under the thigh joint but at the same level, but the animation would still work. However, the animation file and the model must contain the same skeleton, exactly the same. Like if the skeleton in one of the clip has just one more group node, Unity would not recognize it as the same skeleton as the one in model file, and the clip just shows no motion at all. Unfortunately, there is a bug in game exporter of maya. When you export clip and model, even though the selection is the same for skeleton, the skeletons in both files just don't match. The one in clip file always ignores the skeleton group node at top level, and the one in model always includes it no matter we select the skeleton group node or not. The workaround is use another group node to contain both the mesh and the skeleton group. In this way, the clip file skeleton would include its group node, and both skeletons finally match.

Here is a picture showing the data exported in both ways, and notice the file size proves our point above.![[GetImage.jpeg]]

In maya, we can use game exporter tool or export selection tool to achieve the exporting opertation. Both provides similar options.![[GetImage (1).jpeg]]

Geometry section is very self explaining. Smoothing group, tangents&bionormals, and skinning are needed for most of the time. Skinning has to be checked, otherwise the exported model would lose all the skin weight information. BlendShapes can be checked if we are using it, and Unity accepts this kind of deformer. In the settings, move to Origin can help you move the selection object back to world origin when exporting, saving you from doing it manually. Animation would be checked if we want to bake the animation data to the skeleton. Input connections is checked for exporting locators or things that are not joint, and game engine usually dislikes this kind of things. Embed media means the texture applied on the mesh, checking it would automatically export the texture as well, convenient but we normally export it separately just to make things organized and clean. And yes, FBX is the format we use for exporting model and animation clips to game engines.

![[GetImage (2).jpeg]]

Animation clips are automatically detected? We can select which clip to export and modify the frame manually, but overall, it is similar to the export setting for model. 

Humanoid Root Transform 

If the animation clip includes movement of the root joint, and motion extraction is not an option as the animator doesn't know how to do it in Maya, then there is a good chance that the character would slide/skid in the game. Unity provides a set of tool to apply root transform based on the animation, so that the root of the character in game world is moving just like what the root joint does in the animation clip. It is the opposite of motion extraction as in motion extraction, we zero the root joint movement in the clip, and assign the root of the character in game manually to match the clip. In root transform, we need the clip to have root joint movement, and the tool in Unity would take that information, modify it if necessary, then apply it to the root of the character in game.  

Avatar in Unity 

For root transform to work, we need to utilize another Unity tool called avatar. It turns out a generic avartar also works for root transform, not necessarily with a humanoid avartar. To see the root transform options, we need to properly set the root motion node. This avatar tool only works for humanoid for now, and it is basically a general version of humanoid rig in Unity. As we know, a clip can apply to different characters as long as they share the same rig. However, sometimes, a character is a modified version of another, and it knows a few additional moves and perhaps has some differences as well. If we directly apply the same clip, it might not work as there are differences in joints. In cases like this, we can use this avatar layer to syncronize both rigs, by assigning the same joints to the same slots and ignoring the different ones. Then the clip would work as now it only sees the same joints. This process is called animation retargeting. But this avatar tool serves more than one purpose, like root transform needs it in order to work. 

Setting up avatar 

When we import a model into Unity, in the import settings, we can go to the rig section, and define the avatar there. Currently, this system only works for humanoid, so we also need to set the animation type as humanoid, otherwise we cannot configure the avatar.

![[GetImage (3).jpeg]]

Into the configuration window, we need to assign the corresponding joint to the avatar. This dwarf rig has center joint locating in the area of hips, and this joint is the ultimate parent joint in skeleton. It is different from a skeleton where the ultimate parent joint is root joint that locates betweeen two legs and on the ground. How an ordinary skeleton matches this avatar system still needs more experiments.

![[GetImage (4).jpeg]]

Now, back to root transform for animation clips, with the avatar set, we can go to the animation clip's import settings. In the rig section, we can tell it to reference the avatar just created. It turns out a generic avartar also works for root transform, not necessarily with a humanoid avartar. To see the root transform options, we need to properly set the root motion node. Normally, if the avartar has a proper root node assigned, then we don't need to assign anything for the root node in the clip. An ideal setup for root node is a ultimate parent joint called root joint, which is above the hip joint and positioning in the middle of the legs but right on the ground with translate y equalling to zero. When we record the animation, the root joint is moving along but staying on the ground, giving a transform information to drive our collider or root motion or whatever.

![[GetImage (5).jpeg]]

Then, in the animation section, we would have root transform rotation, position (Y), and position (XZ) parameters which would not be there if the avatar setting is none.

![[GetImage (6).jpeg]]

These parameters are here to apply modification of the root joint movement information in the animation clip before assignint it to the the root transform of the character. By adjusting them, we can make the transform of the root joint be affected by the animation in way that we want. It is the opposite of motion extraction. Motion extraction kills the root joint movement in Maya, and assigns the movement to transform manually in Unity. This root transform tells the transform in Unity to follow the root joint movement in Maya instead. 

A little test is done about the accuracy of the root transform. 

We use a walk clip with motion to test the root transform of Unity to see if the distance in clip and the distance in Unity are roughly the same. We use 1-meter cube as our reference object, and the result is pretty accurate. So, if we merely use root transform technique to control the character's movement, we only have the clip as our adjustable control, and the more accurate the root transform technique is, the better we can control the character's movement through the clip. So far, it seems the accuracy level is good enough for this kind of workflow to be possible. 

Workflows between root transform and motion extraction 

The applying root transform function would make the game object move according to the played animation, and normally that means you don't need to program the movement in script, you just program the input to affect the animation parameters so the character would play the appropriate animation clips based on the player's input. However, we can no longer control the character's movement in script, we invoke the clip and how the character moves is controlled by the clip. To change the movement, we need to go back to Maya to change the clip itself. 

On the other hand, if we use motion extraction technique, we would have control of the character's movement in script. However, in order to match the clip and the game object's movement, we still need to adjust either of them. From a designer's perspective, we should have a good idea of how fast the character can move, as this is closely related to the time it needs to traverse the game world, and this time affects the playthrough time of the game. Furthermore, the art team would design the game world in a correct size based on the playthrough time. Therefore, we should know how the character moves before we work on the animation clip of locomotion. With that info ready and fixed, we are left with only one option which is to adjust the clip to match whatever movement that we design for this character relative to the whole game, no matter which workflow you choose. 

Maybe both? 

We can still apply translate on transform component or move on CharacterMovement component while using root transform. A little test is done about this. Two identical characters have root transform turned on, and one is with additional translate or move applied on its components, the other is without. The result is the one with additional movement travels a longer distance. Therefore, we can actually apply adjusting movement (positive or negative) through script on top of what we have with the root transform. For most of time, the root transform helps planting the foot on the ground, which is quite important. We can assign animation clips with motion and clips without motion to a character with root transform turned on, and for those clips without motion, we simply apply the movement through script, right? However, it seems clips without motion under root transform can still cause character to move by the floating animation parameter. If we put the clips without motion out of the blend tree, so it can be invoked by animation parameters which are not float. For example, like bool, the character would not move even with root transform turned on. 

Animation tool in Unity 

Unity also provides a set of animation tool similar to what we have in Maya, but less polished. The reason why we want to make animation directly in Unity, would be sometimes, when we have the whole scene in Unity, it is easier to make certain animation clips. For example, a box is picked up by a crane and moved to somewhere else. If we don't have the whole scene, it is difficult to know the transform for the box. And, we normally build the scene in Unity, not in Maya. Making animation in Unity would first match the scene perfectly, secondly save us from bringing the scene into Maya. With that being said, we still want to make most animation clips in Maya as Maya has a set of stronger tool for animation, meaning we can get a good clip in a much faster speed. The conclusion is this, making animation clips in Maya when those clips are somewhat complex, and they are independent on other objects. Otherwise, we can consider doing it in Unity. 

Unity object hierarchy 

In Unity, when we build an object, normally, it has one parent object which has all the logical components like script, rigidbody, collider, etc. And it also has one or more child objects for the visual representation, and they have components like the mesh, mesh renderer, etc. Also, animator component normally should be attached to the parent object as it can access the child objects for their parameters, but if it is attached to the child object directly, the animator has no access to other child objects. The game would still run if there is no visual representation objects, it is just not playable as player cannot see it, but to the computer, all the necessary parts are there. Thinking it this way explains the way we build an object in Unity, we are separating the logical game object and its visual representations apart using a parent-child hierarchy. 

And all objects, no matter it is a parent or child, they all have transform components. Seperating visual representation objects gives us the ability to adjust their transforms differently. For example, a tank would have two visual objects, one for the upper cannon, and one for the wheels. We only need to make one animation clip for firing cannon, and we can adjust the cannon object's rotation transform to achieve the visual in which the cannon is rotating. It is so much easier and more efficient than doing this in animation clip. 

Unity's animation system architecture is very similar to the one described in GameEngine. It has three levels: the low-level animation pipeline, the mid-level action state machine, and the high-level animation controller. Unity offers an attaching component called animator for game object that needs animation. This animator component has a slot called animator controller, and we can assign different controllers for the same game object, even in runtime. This animator controller in Unity is exactly what the high-level animation controller is. One animator controller contains the mid-level action state machine, and the low-level animation pipeline, by assigning different animator controllers to the same game object, we can assign different sets of animation logics to the same game object. For example, pressing w would make the character run forward, but if the character is in a vehicle, pressing w would make the vehicle drive forward instead of running. The mid-level state machine is exactly what the Unity offers when we open the animation controller in the animator window. By default, we have two states, any state and entry. We can create our own states by dragging animation clips into the animator window. The first one created would automatically become the default state. Transitions between states are also defined here, whether they are based on the finish of a clip or a parameter we define, or even exactly how to transit. When we select a clip or a transition in the animator window, we can see the related information in the inspector window and modify it. The goal or the job here, is to design a set of logics for animation clips to connect each other. And, we have blend tree to blend multiple but closely realted clips together. We have animation layer to help isolate and simplify animation logics. And avatar masks to help blending between animation layers, of course with avatar being set up first. The low-level animation pipeline is coded by Unity, and we can only participate through all those pre-defined interfaces. 

Blend Tree in Unity offers a couple of different blend types. We can understand those blend types in GameEngine and run a example in Unity to see each of them in action. The blend tree has a couple of blend types to determine how to blend multiple clips together, but they all use float parameters. 1D blend type means it uses single float parameter to determine which clip should be played. By default when you create a blend tree, it automatically uses 1D and create a float parameter called blend. And we need to add those clips in motion slots. 2D blend types means it uses two float parameters to determine which clip should be played, and there are a couple different 2D blend types. Need more research to figure out which one is good for what. 

Apply runtime rigging. Alter the animation clips to better suit our needs during runtime. Post-poccesssing techniques are examples of runtime rigging. In GameEngine, procedural animation is any animation generated at runtime. Both runtime rigging and procedural animation are about the same topic. How to do it in Unity? 

Foot IK option where we can find in animator controller panel when we click one of its clips, has very slight affect (according to the Unity blog about Mecanim Humanoids, good to check out more related Unity blogs), and most of the time, the affect is too slight to be actually considered working. And this option underneath utilizes the avatar setup, so if we don't set up an avatar for the clip and model, turning it on would not have any affect at all, not even slight affect. Therefore, we implement the IK affect through another way, the animation layer IK pass. 

The animation layer pass option is in the layer setting of each animation layer. By turning it on, we are effectively putting all the clips in this layer under the affect of the avatar IK system. So far, it looks very similar to the Foot IK option for each clip, but this works for the whole animation layer. Well, there is more. A function named OnAnimatorIK() is called every frame like Update(), and we would define our own IK behaviors in this function.  

First, we would need to set the blending factor between IK pose and the clip pose, using animator's API called SetIKPositionWeight and SetIKRotationWeight. Both APIs take in two parameters, the IK object, and the weight value (maybe more, check manual). For the IK object, we need to use AvatarIKGoal.LeftFoot. This AvatarIKGoal is part of the build-in avatar IK system, and it also provides right foot and both hands as options. The weight value is an normalized value ranging from zero to one. Because this method also utilizes the avatar setup, it only works on humanoid avatar, and by default it only works for the hands and feet. Also, by hands and feet, it means the joints assigned to the hand slots and foot slots during the avatar configuration. 

Then, we have two more APIs: animator.SetIKPosition() and animator.SetIKRotation(). These two APIs also take in the IK object as their first parameter, and a position value for SetIKPosition(), a rotation value in Quaternion for SetIKRotation(). We use these two APIs to give the IK object a new position and rotation, then the IK object blends with the animation pose based on the weighted value. So far, it is quite straightforward. The tricky part is about how to get the new position and rotation for our IK object so that it behaves as truly inverse kinematics, because so far, we just overwrite certain joints' position and rotation values in the animation clips, and there is nothing IK about that. 

IK indicates the movement is evaluated from the end point, and in the example of a foot, this end point is where the foot touches whatever the foot steps on. The only way we can find out if two things collide with each other in Unity, is to use physics. So, the stepped object must have a collider for this to work. Next, we use Physics.Raycast() to find out that foot landing point. In the example of walking, the ray would originate at the position of the foot joint, but with a higher y value in case of an object is higher than the foot but still within the possible range of leg motion. The build-in animator.GetIKPosition() would return the position of the IK object. The direction of the ray is straight down. The distance of the ray can be taken care in raycast. Then, we Physics.RayCast() to fire the ray and store the hit in a RaycastHit object. We feed the Physics.RayCast() with the ray object we define, with the RaycastHit object we establish, and distance, and a layerMask. The distance is the sum of a variable we can define to describe the distance from the foot joint to the bottom of the foot, plus the y adjusting value we used for originating our ray object. This distance is the farthest because any landing point farther than this distance would mean we are stretching our leg, and that is not what we want. The layermask simply allows us to take in or ignore possible raycast hit based on the layer the object is in. Like the character is very likely to have a collider and from where we fire our ray, the chances are the ray would hit the character's collider first before it hits anything else, so the character can be put into a layerMask which the raycast ignores. 

Next, we access the raycast hit object for information. Its point attribute gives us the position where ray hits the collider. Its transform.position attribute gives us the position of the collider-attaching object. And, we want the hitting point. The hitting point is where the foot should land, but the IK object is the foot joint, so the hitting point's y value should be added by the variable we once use in Physics.RayCast() to describe the distance between the foot joint and the bottom of the foot. Now, this new hitting point position can be used in the SetIKPosition().  

For the rotation, we use Quaternion.LookRotation() to generate a quaternion value based on a forward and an upward vector we provide. We provide transform.forward as the forward vector, then we access the raycast hit object's normal attribute to obtain the normal direction of the corresponding face on that object. That's it, we have implemented the IK behavior for the foot. 

But wait, the foot IK behavior is active all the time. And maybe in some clips like walking, the foot needs to be slightly off the ground during a portion of the clip, but this IK behavior is putting the feet on the ground the whole time. So, we use a float animation parameter to be the IK blending factor, and for the clip, we go to the inspector out of the animation panel, we can see curves option. We can add the float animation parameter to the curve option by typing the name of the parameter, then configure the values throughout the whole clip. This tool allows us to change the parameter value for different parts of the clip, and we use the parameter for our need, in this case, turning on and off the IK behavior. Lastly, we go back to the SetIKPositionWeight() and SetIKRotationWeight(), and use the parameter as the weight value.