Unity offers three types of render pipelines: the default build-in render pipeline, the universal render pipeline (URP), the high definition render pipeline (HDRP). HDRP is designed for games running on high-end hardware. URP is designed for the rest. The default one is legacy but it is very similar to URP  in terms of graphic level. However, URP and HDRP gets to use the Unity build-in shader graph. To use URP and HDRP, we need to have create the URP asset or HDRP asset by clicking the plus sign in project window. Then, go to project setting/graphic, assign the asset. 

Those three render pipelines reqiure different shaders for mesh to display correctly, otherwise, the mesh would go magenta. The default one requires standard. URP requires URP shaders, and HDRP requires HDRP shaders. We can either go to Edit/render pipeline/URP/upgrade selected materials or upgrade project materials at the same menu location. For materials with a shader that is not standard (for the default pipieline), upgrade project materials doesn't work correctly, and we need to search for material name in the project panel and select them, then choose the correct shader in the inspector directly. 

With URP and HDRP, they provide progress lightmapper to replace the deprecated Enlighten lightmapper. If settings are correct, they should have a better graphic level when baking lightmap. However, they are something to watch out no matter what lightmapper is used when baking lightmap. 

Mesh with mesh renderer component has castShadow parameter. It decides the light use either the front face or both faces of the mesh to cast shadows. Since we often delete the unseen faces of the mesh to save polygon counts, leaving the backface of the polygon exposed to the source of the light, if only using front face to cast shadow, we would see light shines through the mesh as if there are no mesh at all. 

A mesh is static for global illumination means it is used to calculate the lightmap for static indirect lighting. Itself would receive indirect lighting from other meshes as well. A static mesh should normally have global illuminaion static checked for better graphic fidelity. A mesh sometimes has parts that are not vertice connected, when light shines through it , the shadow might have gaps for Unity recognizes the mesh is not connected for that part. The way to fix this, is either to adjust the normal bias under the URP asset or just remodel the mesh in maya. 

In the URP asset, there is a group of parameters called shadwos in the inspector. It affects mainly real-time lighting not baked lighting. Distances means how far from the camera should render shadow. With value lower, the rendering distance is lower. And meshes further away would render without shadow. Cascades are basically the LOD version for shadow, the further away, the quality of the shadow is lower. 

One of the key factor for baking good lightmap is the lightmap UV. It is a separate set of UV for the lightmapper to decide where to have light or shadow. We could either tell Unity to generate lightmap UV when import the mesh or we could do the UV unwrap in maya. When select mesh prefab, we can see there is a check box for generate lightmap UV in the inspector. With that checked, Unity would generate lightmap UV and use it. However, it is usually a better choice to do the UV unwrap in maya since we need to do it anyway for texturing.  

Area light placement is important. If it is placed within a mesh partially, only the part that is outside the mesh would be taken into light baking. Therefore, we normally should avoid placing the area light inside a mesh. 

There is a technique called mipmaping. Definition from the internet: UV maps are a 2d position relative to the texture's bounds. The dimensions or aspect ratio of the texture have no affect on the UVs. A UV position of 0.5, 0.5 is the middle of the texture if it's 2048x2048, or 4x4, or 1280x96. As for mip maps, imagine you have a 256x256 texture with a lot of details in it. Now imaging it's being displayed on screen at only 64x64 pixels. That's 1/4th the original resolution. With out mip maps the GPU is basically going to render 1 out of every 16 pixels from the original texture as one on screen pixel covers an area of 4x4 pixels. This results in a lot of aliasing as many of the details from the original texture are simply missed. Mip maps handle this case by having a version of the texture that's already 64x64 with each pixel color being an average of those 4x4 pixel areas from the original larger texture. 

Unity uses mipmaping as well for texture and lightmap. If a mesh presents a shadow where there shouldn't be any, especially at the edge or UV seam. It might be the side effect of mipmapping. Now, we could either reduce the resolution of the UV to avoid the mipmapping at all or we could add more texture space between each UV shell so when mipmapping happens, it would not have a noticeable impact. 

ColorSpace is a parameter you will find at build setting/player setting/other setting/rendering. It has two values: Gamma or Linear. Computers usually store lighting information in a sRGB (gamma corrected) fashion. Cameras get the information in a linear fashion (the more light hit the sensor of the camera the higher the number it stores, linearly). Human perception is closer to a logarithmic curve, we see smaller changes in the darker areas and less in the brighter ones (evolutionary advantages: we saw the predators move in the dark so they didn't eat us). The gamma you see in unity settings means that the settings and the textures you're using was prepared in this gamma-corrected colorspace, every change was made in it. The linear colorspace setting roughly means that you store the lighting information as is, you make your changes in physically closer to the realistic colors, so you can apply realistic effects and you convert the results once before you show to the viewers. You choose if you want Unity to store your lightning already in gamme corrected (Gamma). Or you can choose to store in Linear (Linear). Some devices don't support Linear (usually older or some mobiles). If you choose Gamma, you get more "plastic"-like colors at the end, because everything is happening in gamma-correcter color space. All your changes, effects, etc. Basically Unity store everything in srgb and just shovel it out to the GPU. If you choose Linear, everything will happen in physically more correct fashion (this is why the screenshot in Linear color space are more "natural"), so we can apply effects on more realistic lighting values. And the gamma correction will happen at the end, when everything is set (so the monitor can display the image). It's not perfect, so you can't apply everything physically real (like for example sun light), but it's close enough. Obviously Linear is a little bit more resource-hungry, so better mobiles or better desktops usually support this or higher. It is not mandatory to use one or the other, just pick the one that fits your game and targeted platform. 

Now, bake lightmap setting.
![[UnityDevLog/LightMapping/GetImage.png]]
![[UnityDevLog/LightMapping/GetImage (1).png]]